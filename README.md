
# Risk Insight Copilot: Financial Risk Analyzer

**Course:** SEP 769 ‚Äì Cyber Physical Systems  
**Group Members:** Kushal Shah, Jenil Virani, Sarthak Paliwal  
**Term:** Summer'25 

## Project Overview

The ‚ÄòFinancial Risk Copilot‚Äô is an AI powered financial analyst that reviews 10-Ks and determines the risk level of any publicly listed company. This idea stems from the issue of individual investors and beginner analysts having a hard time analyzing millions of data points to accurately identify the risk associated with any investment.
 
## Repository Structure

```
financial-risk-analyzer/
‚îÇ
‚îú‚îÄ‚îÄ env/                          # Environment configuration (environment.yml)
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                   # All project notebooks (step-by-step pipeline)
‚îÇ   ‚îú‚îÄ‚îÄ STEP_1_10-K_Extraction.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ STEP_2_10-K_sort.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ STEP_3_Item1A_Item7_extraction.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ STEP_4_convert_to_parquet.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ STEP_5_LangChain_Integration.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ STEP_6_DL_data_cleaning.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ STEP_7_Feature_eng_text_embeddings.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ STEP_8_Parquets_division_batchWise.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ STEP_9_Modelling.ipynb
‚îÇ
‚îú‚îÄ‚îÄ .gitignore                   # Hidden files & API keys excluded from repo
‚îî‚îÄ‚îÄ README.md                    # This file
```

---

## Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/kushalshah1303/financial-risk-analyzer.git
cd financial-risk-analyzer
```

### 2. Create Conda Environment

Make sure you have Anaconda installed, then run:

```bash
conda env create -f env/environment.yml
conda activate financial-risk-analyzer
```

### 3. API Key Configuration

To run the LangChain+Groq integration, create a `.env` file with the following format:

```
GROQ_API_KEY=your_actual_api_key_here
```

> ‚ö†Ô∏è The `.env` file is excluded in `.gitignore` for privacy.

## Notes

- Due to token limits, only ~600 rows were processed in the LLM step (free Groq API cap).
- Data files (`.parquet`, `.csv`) and large model outputs are not uploaded due to size, but can be generated when the code is run step by step.
- All notebooks are clean, commented, and execute without error under the provided environment.


## üß™ How to Run the Code

Execute the notebooks in the following sequence for full pipeline execution:

1. `STEP_1_10-K_Extraction.ipynb` ‚Äì Download and extract 10-K filings  
2. `STEP_2_10-K_sort.ipynb` ‚Äì Organize and filter filings  
3. `STEP_3_Item1A_Item7_extraction.ipynb` ‚Äì Extract sections of interest  
4. `STEP_4_convert_to_parquet.ipynb` ‚Äì Save structured data  
5. `STEP_5_LangChain_Integration.ipynb` ‚Äì Extract risk narratives using LLM  
6. `STEP_6_DL_data_cleaning.ipynb` ‚Äì Clean and structure LLM outputs  
7. `STEP_7_Feature_eng_text_embeddings.ipynb` ‚Äì Generate FinBERT embeddings  
8. `STEP_8_Parquets_division_batchWise.ipynb` ‚Äì Efficient memory handling  
9. `STEP_9_Modelling.ipynb` ‚Äì Train and evaluate neural network model


## üõ† Dependencies

- Python 3.10+
- LangChain
- Transformers (FinBERT)
- Torch
- Pandas, NumPy, Matplotlib, Seaborn
- BeautifulSoup, Regex, tqdm
- dotenv (for API handling)

See `env/environment.yml` for full dependency list.

## Contact

For any questions or setup issues, please reach out to us.
